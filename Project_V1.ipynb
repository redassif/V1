{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We define the Needle-in-Sequence environment as a finite episodic MDP where the state consists of the observed sequence and hidden needle index. The reward function is deterministic. Although the transition function is easy to compute in this environment, we explicitly represent it to enable future model-based RL. The agent maintains estimates of\n",
        "ð‘…(ð‘ ,ð‘Ž) and ð‘ƒ(ð‘ â€²âˆ£ð‘ ,ð‘Ž) to allow dynamic programming."
      ],
      "metadata": {
        "id": "RQtltAN1bpBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "           â”‚     Environment           â”‚\n",
        "           â”‚  X, hidden index          â”‚\n",
        "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                         â”‚\n",
        "                         â–¼\n",
        "                 Observation: X\n",
        "                         â”‚\n",
        "                         â–¼\n",
        "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "               â”‚      Agent     â”‚\n",
        "               â”‚ Hierarchical   â”‚\n",
        "               â”‚   Attention    â”‚\n",
        "               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "                 Action: pick index a\n",
        "                       â”‚\n",
        "                       â–¼\n",
        "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "           â”‚ Reward: r = 1 if a==i     â”‚\n",
        "           â”‚         r = 0 otherwise   â”‚\n",
        "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ],
      "metadata": {
        "id": "q6PjolB1i0rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class NeedleInSequenceEnv:\n",
        "    \"\"\"\n",
        "    Observation: X in R^{T x d}\n",
        "    Hidden needle index i*\n",
        "    Action: choose index a in {0..T-1}\n",
        "    Reward: 1 if a==i* else 0\n",
        "    Episode length: 1 step\n",
        "    \"\"\"\n",
        "    def __init__(self, T=64, d=16, signal_strength=6.0, device=\"cpu\"):\n",
        "        self.T = T\n",
        "        self.d = d\n",
        "        self.signal_strength = signal_strength\n",
        "        self.device = device\n",
        "        self.X = None\n",
        "        self.needle_idx = None\n",
        "        self.needle_vec = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.X = torch.randn(self.T, self.d, device=self.device)\n",
        "        self.needle_idx = int(torch.randint(0, self.T, (1,), device=self.device).item())\n",
        "        self.needle_vec = torch.randn(self.d, device=self.device)\n",
        "        self.X[self.needle_idx] += self.signal_strength * self.needle_vec\n",
        "        return self.X.clone()\n",
        "\n",
        "    def step(self, action: int):\n",
        "        reward = 1.0 if action == self.needle_idx else 0.0\n",
        "        done = True\n",
        "        info = {\"needle_idx\": self.needle_idx}\n",
        "        return self.X.clone(), reward, done, info\n",
        "\n",
        "\n",
        "class HierarchicalAttentionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses Blocks in a hierarchy, to determine which tokens would be attended using a top-k approach\n",
        "      - block summaries\n",
        "      - select top-k blocks\n",
        "      - attend within chosen blocks (top-k tokens)\n",
        "    \"\"\"\n",
        "    def __init__(self, d: int, block_size: int = 8, k_blocks: int = 2, k_tokens: int = 4):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.block_size = block_size\n",
        "        self.k_blocks = k_blocks\n",
        "        self.k_tokens = k_tokens\n",
        "\n",
        "        self.Wq = nn.Linear(d, d)\n",
        "        self.Wk = nn.Linear(d, d)\n",
        "        self.Wv = nn.Linear(d, d)\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        T, d = X.shape\n",
        "        assert d == self.d\n",
        "        assert T % self.block_size == 0,\n",
        "\n",
        "        Q = self.Wq(X)\n",
        "        K = self.Wk(X)\n",
        "        V = self.Wv(X)\n",
        "\n",
        "        num_blocks = T // self.block_size\n",
        "\n",
        "\n",
        "        K_blocks = K.view(num_blocks, self.block_size, d)\n",
        "        V_blocks = V.view(num_blocks, self.block_size, d)\n",
        "\n",
        "\n",
        "        block_keys = K_blocks.mean(dim=1)\n",
        "\n",
        "        contexts = []\n",
        "\n",
        "        for i in range(T):\n",
        "            q = Q[i]\n",
        "\n",
        "            # Choose top-k blocks\n",
        "            block_scores = (q @ block_keys.T) / math.sqrt(d)\n",
        "            chosen_blocks = torch.topk(block_scores, self.k_blocks).indices\n",
        "\n",
        "            c_i = torch.zeros(d, device=X.device)\n",
        "\n",
        "            # Local attention\n",
        "            for b in chosen_blocks:\n",
        "                Kb = K_blocks[b]\n",
        "                Vb = V_blocks[b]\n",
        "\n",
        "                token_scores = (q @ Kb.T) / math.sqrt(d)\n",
        "                chosen_tokens = torch.topk(token_scores, self.k_tokens).indices\n",
        "\n",
        "                w = F.softmax(token_scores[chosen_tokens], dim=0)\n",
        "                c_i += (w.unsqueeze(-1) * Vb[chosen_tokens]).sum(dim=0)\n",
        "\n",
        "            contexts.append(c_i)\n",
        "\n",
        "        c = torch.stack(contexts, dim=0).mean(dim=0)  # (d,)\n",
        "        return c\n",
        "\n",
        "\n",
        "class HierAttnPolicyValueNet(nn.Module):\n",
        "\n",
        "    def __init__(self, d: int, T: int, block_size=8, k_blocks=2, k_tokens=4, hidden=128):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.encoder = HierarchicalAttentionEncoder(d, block_size, k_blocks, k_tokens)\n",
        "\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear(d, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, T)\n",
        "        )\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(d, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X: torch.Tensor):\n",
        "\n",
        "        c = self.encoder(X)                # (d,)\n",
        "        logits = self.policy_head(c)       # (T,)\n",
        "        value = self.value_head(c).squeeze(-1)  # scalar\n",
        "        return logits, value\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AgentArgs:\n",
        "    T: int = 64\n",
        "    d: int = 16\n",
        "    block_size: int = 8\n",
        "    k_blocks: int = 2\n",
        "    k_tokens: int = 4\n",
        "\n",
        "    signal_strength: float = 6.0\n",
        "    lr: float = 1e-3\n",
        "    episodes: int = 5000\n",
        "    eval_episodes: int = 1000\n",
        "    seed: int = 0\n",
        "    device: str = \"cpu\"\n",
        "\n",
        "    artifacts_dir: str = \"artifacts\"\n",
        "    run_name: str = \"needle_hierattn_rl\"\n",
        "\n",
        "\n",
        "class HierAttnRLAgent:\n",
        "    \"\"\"\n",
        "    Trains a hierarchical-attention policy with reinforcement baseline\n",
        "    Saves best policy and weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, args: AgentArgs):\n",
        "        self.args = args\n",
        "        torch.manual_seed(args.seed)\n",
        "\n",
        "        self.env = NeedleInSequenceEnv(\n",
        "            T=args.T, d=args.d, signal_strength=args.signal_strength, device=args.device\n",
        "        )\n",
        "\n",
        "        self.model = HierAttnPolicyValueNet(\n",
        "            d=args.d, T=args.T,\n",
        "            block_size=args.block_size, k_blocks=args.k_blocks, k_tokens=args.k_tokens\n",
        "        ).to(args.device)\n",
        "\n",
        "        self.opt = torch.optim.Adam(self.model.parameters(), lr=args.lr)\n",
        "\n",
        "        os.makedirs(args.artifacts_dir, exist_ok=True)\n",
        "        self.ckpt_prefix = os.path.join(args.artifacts_dir, args.run_name)\n",
        "\n",
        "        self.best_score = -1.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act(self, X: torch.Tensor, greedy: bool = False) -> int:\n",
        "        logits, _ = self.model(X)\n",
        "        if greedy:\n",
        "            return int(torch.argmax(logits).item())\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        a = int(torch.distributions.Categorical(probs).sample().item())\n",
        "        return a\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, episodes: int, greedy: bool = True) -> float:\n",
        "        correct = 0.0\n",
        "        for _ in range(episodes):\n",
        "            X = self.env.reset()\n",
        "            a = self.act(X, greedy=greedy)\n",
        "            _, r, _, _ = self.env.step(a)\n",
        "            correct += r\n",
        "        return correct / episodes\n",
        "\n",
        "    def save_best(self) -> None:\n",
        "        torch.save(self.model.state_dict(), self.ckpt_prefix + \"_best.pt\")\n",
        "        meta = {\"best_score\": float(self.best_score), \"args\": asdict(self.args)}\n",
        "        with open(self.ckpt_prefix + \"_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, indent=2)\n",
        "\n",
        "    def load_best(self) -> bool:\n",
        "        path = self.ckpt_prefix + \"_best.pt\"\n",
        "        if not os.path.exists(path):\n",
        "            return False\n",
        "        self.model.load_state_dict(torch.load(path, map_location=self.args.device))\n",
        "        meta_path = self.ckpt_prefix + \"_meta.json\"\n",
        "        if os.path.exists(meta_path):\n",
        "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                meta = json.load(f)\n",
        "            self.best_score = float(meta.get(\"best_score\", self.best_score))\n",
        "        return True\n",
        "\n",
        "    def train(self) -> Dict[str, Any]:\n",
        "\n",
        "        for ep in range(1, self.args.episodes + 1):\n",
        "            X = self.env.reset()\n",
        "\n",
        "            logits, value = self.model(X)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            a = dist.sample()\n",
        "\n",
        "            _, r, _, _ = self.env.step(int(a.item()))\n",
        "            reward = torch.tensor(r, device=self.args.device, dtype=torch.float32)\n",
        "\n",
        "            advantage = (reward - value.detach())\n",
        "\n",
        "            policy_loss = -dist.log_prob(a) * advantage\n",
        "            value_loss = F.mse_loss(value, reward)\n",
        "            loss = policy_loss + 0.5 * value_loss\n",
        "\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "            if ep % 200 == 0:\n",
        "                score = self.evaluate(self.args.eval_episodes, greedy=True)\n",
        "                if score > self.best_score:\n",
        "                    self.best_score = score\n",
        "                    self.save_best()\n",
        "\n",
        "                print(f\"ep={ep:5d}  loss={loss.item():.4f}  eval_acc={score:.3f}  best={self.best_score:.3f}\")\n",
        "\n",
        "        final_score = self.evaluate(self.args.eval_episodes, greedy=True)\n",
        "        return {\"final_eval_acc\": final_score, \"best_eval_acc\": self.best_score}\n",
        "\n",
        "\n",
        "def run(mode: str, args: AgentArgs):\n",
        "    agent = HierAttnRLAgent(args)\n",
        "\n",
        "    if mode == \"train\":\n",
        "        out = agent.train()\n",
        "        print(\"[TRAIN DONE]\", out)\n",
        "\n",
        "    elif mode == \"eval\":\n",
        "        if not agent.load_best():\n",
        "            raise RuntimeError(\"No saved best model found. Run train first.\")\n",
        "        score = agent.evaluate(args.eval_episodes, greedy=True)\n",
        "        print(\"[EVAL] acc:\", score, \"best_score:\", agent.best_score)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'train' or 'eval'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = AgentArgs(\n",
        "        T=64, d=16,\n",
        "        block_size=8, k_blocks=2, k_tokens=4,\n",
        "        signal_strength=6.0,\n",
        "        lr=1e-3,\n",
        "        episodes=3000,\n",
        "        eval_episodes=500,\n",
        "        device=\"cpu\",\n",
        "        run_name=\"needle_hierattn_rl_v1\"\n",
        "    )\n",
        "\n",
        "    run(\"train\", args)\n",
        "    run(\"eval\", args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHtfwV1vcWa2",
        "outputId": "ebf91c64-b8f2-4cb3-fcdc-51fec815b3c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep=  200  loss=-0.8545  eval_acc=0.016  best=0.016\n",
            "ep=  400  loss=-0.5232  eval_acc=0.014  best=0.016\n",
            "ep=  600  loss=-0.4469  eval_acc=0.010  best=0.016\n",
            "ep=  800  loss=0.2501  eval_acc=0.012  best=0.016\n",
            "ep= 1000  loss=0.1940  eval_acc=0.010  best=0.016\n",
            "ep= 1200  loss=0.0709  eval_acc=0.022  best=0.022\n",
            "ep= 1400  loss=-0.0241  eval_acc=0.018  best=0.022\n",
            "ep= 1600  loss=0.1074  eval_acc=0.006  best=0.022\n",
            "ep= 1800  loss=-0.0161  eval_acc=0.010  best=0.022\n",
            "ep= 2000  loss=-0.2611  eval_acc=0.012  best=0.022\n",
            "ep= 2200  loss=-0.0409  eval_acc=0.016  best=0.022\n",
            "ep= 2400  loss=-0.2793  eval_acc=0.020  best=0.022\n",
            "ep= 2600  loss=-0.0238  eval_acc=0.026  best=0.026\n",
            "ep= 2800  loss=-0.0817  eval_acc=0.014  best=0.026\n",
            "ep= 3000  loss=-0.1408  eval_acc=0.020  best=0.026\n",
            "[TRAIN DONE] {'final_eval_acc': 0.006, 'best_eval_acc': 0.026}\n",
            "[EVAL] acc: 0.012 best_score: 0.026\n"
          ]
        }
      ]
    }
  ]
}